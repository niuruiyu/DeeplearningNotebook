{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8670c162-d6b2-454b-99c4-10701dd2c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#继承Module类来构造模型\n",
    "import torch\n",
    "from torch import nn\n",
    "class MLP(nn.Module):\n",
    "    #初始化\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784,256)#隐藏层\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256,10)\n",
    "    #定义前向传播\n",
    "    def forward(self,x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d6940-34b0-4c33-9f90-eb08aa330440",
   "metadata": {},
   "source": [
    "上面的MLP类不用定义反向传播函数，系统会自动求梯度生成backward函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36d29ba-14f5-457c-aaed-3324948a5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0147,  0.0826, -0.1975, -0.2949,  0.0288,  0.0641,  0.1135,  0.1165,\n",
       "          0.0629,  0.1263],\n",
       "        [ 0.0563,  0.0860, -0.1646, -0.0885, -0.1241, -0.0061,  0.0612,  0.1403,\n",
       "          0.0048,  0.2065]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实例化模型\n",
    "net = MLP()\n",
    "#先随机出来数据\n",
    "X = torch.rand(2,784)#两个样本\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc295e1-3ae9-4ce8-b765-f1c40e023e4d",
   "metadata": {},
   "source": [
    "上面没有吧Module类命名维Layer或者Model之类的名字，是因为该类是一个可供自由组建的部件。  \n",
    "它的子类既可以是一个层，也可以是一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60a27af-2a42-42ba-a78b-5a7d184b78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Module的子类\n",
    "#Sequential，ModuleList，ModuleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5c4f1b-114e-4bb8-9356-cce98b2d1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MySequential类的实现，帮助更好的理解Sequential类\n",
    "#Sequential可以接受一个子模块的有序字典，或者时一些列子模块作为参数注意添加Module的实例\n",
    "# 类似于块，就是按照内部的顺序进行前向传播\n",
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    #用来处理有序字典输入\n",
    "    def __init__(self,*args):\n",
    "        #*args允许接受任意数量的参数\n",
    "        super(MySequential,self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0],OrderedDict):\n",
    "            #接受有序字典，当输入是有序字典时遍历字典\n",
    "            for key ,module in args[0].items():\n",
    "                self.add_module(key,module)\n",
    "                #通过add_module方法添加子模块\n",
    "                #每个模块会以字典的键作为名称\n",
    "        else:\n",
    "            #当输入是多个模块时\n",
    "            for idx,module in enumerate(args):#以索引作为模块名添加\n",
    "                self.add_module(str(idx),module)\n",
    "    def forward(self,input):#前向传播方式\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)#遍历所有添加的模块，奖赏一个模块的输出作为下一个模块的输入\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc04630-fe9e-419b-b0ce-25fae4d75fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0896, -0.0396,  0.0170, -0.0694, -0.3118,  0.2313,  0.0015,  0.0560,\n",
       "         -0.0804,  0.0816],\n",
       "        [ 0.2583, -0.0871,  0.0865, -0.2219, -0.2481,  0.2406, -0.0589,  0.1787,\n",
       "         -0.0500, -0.0734]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用上面的MySequential类来实现MLP类\n",
    "net = MySequential(\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e794184-2418-4ba1-b4fe-e7f0f1227e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#ModuleList类接受一个子模块的列表作为输入，也可以类似List那样进行append和extend操作\n",
    "net = nn.ModuleList([nn.Linear(784,256),nn.ReLU()])\n",
    "net.append(nn.Linear(256,10))\n",
    "print(net[-1])\n",
    "#这样创建的net实例就类似于一个列表，可以用索引拿出来\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf17a2d-80af-4837-af89-5735f0177167",
   "metadata": {},
   "source": [
    "可以根据上面两个net的输出看出Sequential和ModuleList都可以列表化构建网络。  \n",
    "ModuleList仅仅是一个存储各种模块的列表，这些模块之间没有联系没有顺序，而且没有实现forward功能  \n",
    "所以不能运行net(x)\n",
    "Sequential内部的模块是实现了顺序的排列（上面的字典一直强调是有序字典），而且内部的forward功能已经实现  \n",
    "所以上面的net(x)没有报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59dbfb6a-8f0d-4fd4-a435-7868e11abcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModuleList是为了让网络的前向传播更加灵活\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(10)])\n",
    "        #创建了一个ModuleList实例linears放10个10*10的全连接层\n",
    "        #用于需要多个相似的层时\n",
    "    def forward(self,x):\n",
    "        for i,l in enumerate(self.linears):#i是索引，l是线性层对象\n",
    "            x = self.linears[i//2](x)+l(x)#i是0~9，这里前面的linears用到的层分别是001122334，后面的l(x)分别是遍历到的线性层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d1c748-f1ac-4c00-b71f-553a2db9a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1:\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "net2:\n"
     ]
    }
   ],
   "source": [
    "#ModuleList不同于list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中\n",
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_ModuleList,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10)])\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_List,self).__init__()\n",
    "        self.linears = [nn.Linear(10,10)]\n",
    "net1 = Module_ModuleList()\n",
    "net2 = Module_List()\n",
    "\n",
    "print(\"net1:\")\n",
    "for p in net1.parameters():\n",
    "    print(p.size())\n",
    "print(\"net2:\")\n",
    "for p in net2.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e524dd6-a0c0-4c89-81db-1c7ae0f11ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#ModuleDict接受一个子模块的字典作为输入，也可以类似与字典那样进行添加和访问操作\n",
    "net = nn.ModuleDict({\n",
    "    'linear':nn.Linear(784,256),\n",
    "    'act':nn.ReLU(),\n",
    "})\n",
    "net['output'] = nn.Linear(256,10)\n",
    "print(net)\n",
    "print(net['linear'])\n",
    "print(net.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246561b-d7a5-4d51-8cf8-29fd8510c49d",
   "metadata": {},
   "source": [
    "和ModuleList一样MOduleDict只是存放模块的字典，并没有实现forward功能，但是添加进去的模块的参数有添加到网路中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "781c8eb9-4289-424f-a8eb-8a6e05c491be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构架一些复杂的网络，利用上面\n",
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP,self).__init__(**kwargs)\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad =False)\n",
    "        #常数权重，不是可训练的模型参数\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = nn.functional.relu(torch.mm(x,self.rand_weight.data)+1)\n",
    "        x = self .linear(x)\n",
    "        #对x进行归一化操作之后返回元素之和\n",
    "        #主要是将张量的norm（范数）控制在特定的范围内\n",
    "        while x.norm().item() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().item() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7ee64c1-a2b7-4c64-b75a-3e18a53750d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1121, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试该模型的前向计算\n",
    "X = torch.rand(2,20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c8608a9-931b-44e5-b286-0aa0e9abeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为FancyMLP和Sequential都是Module的子类，所以可以嵌套调用\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(NestMLP,self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(nn.Linear(40,30),nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc86e86f-dd83-48bb-8bf0-6f1cf2124912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (2): FancyMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.9751, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    NestMLP(),\n",
    "    nn.Linear(30,20),\n",
    "    FancyMLP()\n",
    ")\n",
    "X = torch.rand(2,40)\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef079d-1ada-473d-9eab-87afadb24c2d",
   "metadata": {},
   "source": [
    "总结：  \n",
    "本节介绍了Module构建模型的使用以及三种子类的使用  \n",
    "Sequential，ModuleList，ModuleDict三种子类的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7641999-6650-4805-baf0-e55399733059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
