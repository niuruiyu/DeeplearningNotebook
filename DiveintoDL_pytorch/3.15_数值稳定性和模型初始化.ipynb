{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21352ac-5026-490a-9c85-b56090773cf0",
   "metadata": {},
   "source": [
    "上一届的正向传播和反向传播这里省去了，直接看书就可以"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d59738-0a86-4276-b17d-3aa170ab0839",
   "metadata": {},
   "source": [
    "## 1.衰减和爆炸\n",
    "就是当神经网络层数过多时（因为神经网络的计算是乘积运算）如果一直乘乘n多层，那最后可能会导致输出超级大或超级小（也就是衰减和爆炸），同时由于反向传播过程中，梯度运算（链式法则）（也是乘积运算），那就也有几率会导致梯度的衰减和爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfeab6-9c5c-43ff-991e-aa8c04d9c0fa",
   "metadata": {},
   "source": [
    "## 2.随机初始化模型参数\n",
    "如果隐藏层每个节点的参数都相等，对于相同的输入和相同的优化算法，就会导致相同的参数更新，且之后无论迭代多少次都是一样的，那就会导致很多个节点本质上只能充当一个结点的作用，所以初始化的模型参数需要随机。 \n",
    "这里介绍了两种方法\n",
    "### 默认随机初始化：\n",
    "pytorch的nn.module的模块参数会自动采取合理的初始化策略，一般不用考虑\n",
    "### Xavier随机初始化\n",
    "假设一个全连接层Linear(a,b)  \n",
    "Xavier随机初始化将该层中权重参数的每个元素都随机采样于均匀分布  \n",
    "U(-sqrt(6/(a+b)),sqrt(6/(a+b))  \n",
    "可以保证每层输出的方差不受该层输入个数的影响，梯度的方差不受该层输出个数的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122135f2-db2a-4815-9244-a9368b426e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
